"""
Softmax Multiclass Classification - Preferred Implementation
=============================================================

This script demonstrates the "Preferred" approach to multiclass classification
using Softmax activation with numerical stability considerations.

Educational Objective:
----------------------
Understanding the difference between:
1. The "obvious" approach: output layer with softmax + standard loss
2. The "preferred" approach: output layer with linear + from_logits=True

Key Concept - Numerical Stability:
-----------------------------------
Using from_logits=True is MORE NUMERICALLY STABLE because TensorFlow can
combine the softmax and cross-entropy calculations into a single, optimized
operation that avoids intermediate values that could cause overflow/underflow.

Mathematical Foundation:
------------------------
The Softmax function converts logits (raw scores) into probabilities:

$$a_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}$$

where:
- $z_j$ is the logit (raw score) for class j
- $K$ is the total number of classes
- $a_j$ is the probability for class j
- $\sum_{j=1}^{K} a_j = 1$ (probabilities sum to 1)

Softmax vs ReLU/Sigmoid:
-------------------------
- **ReLU**: $f(z) = \max(0, z)$ - Acts element-wise on EACH neuron independently
- **Sigmoid**: $f(z) = \frac{1}{1+e^{-z}}$ - Acts element-wise on EACH neuron independently
- **Softmax**: $a_j = \frac{e^{z_j}}{\sum_k e^{z_k}}$ - Acts on ALL outputs TOGETHER

The key difference: Softmax is a "coupling" function - it considers all outputs
simultaneously to ensure they form a valid probability distribution.

Sparse vs Categorical Cross-Entropy:
-------------------------------------
**SparseCategoricalCrossentropy**:
- Target labels are INTEGER indices: [0, 1, 2, 3]
- Example: y = [2, 0, 1, 3]
- More memory efficient
- Use when labels are class indices

**CategoricalCrossentropy**:
- Target labels are ONE-HOT ENCODED: [[0,0,1,0], [1,0,0,0], ...]
- Example: y = [[0,0,1,0], [1,0,0,0], [0,1,0,0], [0,0,0,1]]
- Requires more memory
- Use when labels are already one-hot encoded

Author: ML Algorithms from Scratch Project
Date: January 28, 2026
License: MIT
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.optimizers import Adam
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
import os

# =============================================================================
# SECTION 1: DATASET GENERATION
# =============================================================================

def create_and_save_dataset():
    """
    Creates a 4-class dataset using sklearn's make_blobs.
    
    Parameters:
    -----------
    - n_samples: 2000 total examples
    - centers: 4 cluster centers (auto-generated by sklearn)
    - cluster_std: 1.0 (standard deviation of each cluster)
    
    Returns:
        X, y: Feature matrix and labels
    """
    print("=" * 70)
    print("Softmax Multiclass Classification - Preferred Implementation")
    print("=" * 70)
    
    # Create data directory
    os.makedirs('./data', exist_ok=True)
    
    # Generate dataset
    X, y = make_blobs(
        n_samples=2000,
        centers=4,
        cluster_std=1.0,
        random_state=42
    )
    
    # Save to disk
    np.save('./data/X_softmax.npy', X)
    np.save('./data/y_softmax.npy', y)
    
    print("\n✓ Dataset created and saved:")
    print(f"  X shape: {X.shape} (2000 samples, 2 features)")
    print(f"  y shape: {y.shape} (integer labels)")
    print(f"  Classes: {np.unique(y)}")
    print(f"  Class distribution: {np.bincount(y)}")
    
    return X, y

def load_dataset():
    """
    Loads the dataset from .npy files.
    """
    print("\nLoading dataset from files...")
    X = np.load('./data/X_softmax.npy')
    y = np.load('./data/y_softmax.npy')
    
    print(f"  X: {X.shape}")
    print(f"  y: {y.shape}")
    
    return X, y

# =============================================================================
# SECTION 2: CUSTOM SOFTMAX IMPLEMENTATION
# =============================================================================

def my_softmax(z):
    """
    Custom NumPy implementation of the Softmax function.
    
    Mathematical Formula:
    ---------------------
    For an input vector z = [z₁, z₂, ..., zₖ]:
    
    $$a_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}$$
    
    Key Properties:
    ---------------
    1. Output is a probability distribution: all values in [0, 1]
    2. Sum of outputs equals 1: Σ(aⱼ) = 1
    3. Monotonic: higher logits → higher probabilities
    
    Softmax vs Other Activations:
    ------------------------------
    - **ReLU/Sigmoid**: Act ELEMENT-WISE on each neuron independently
      Example: ReLU([2, -1, 3]) = [2, 0, 3]
      
    - **Softmax**: Acts on ALL outputs TOGETHER as a group
      Example: Softmax([2, -1, 3]) ≈ [0.24, 0.01, 0.75]
      
    This "coupling" behavior makes Softmax ideal for classification because:
    - It creates a valid probability distribution
    - Competing classes influence each other's probabilities
    - The highest logit gets the highest probability (but not necessarily 1.0)
    
    Parameters:
        z (ndarray): Input logits, shape (n_classes,) or (batch_size, n_classes)
        
    Returns:
        a (ndarray): Softmax probabilities, same shape as z
    """
    # Numerical stability: subtract max before exponentiating
    # This prevents overflow without changing the result
    # Because: exp(z - max) / sum(exp(z - max)) = exp(z) / sum(exp(z))
    z_shifted = z - np.max(z, axis=-1, keepdims=True)
    
    # Compute exponentials
    exp_z = np.exp(z_shifted)
    
    # Normalize to get probabilities
    a = exp_z / np.sum(exp_z, axis=-1, keepdims=True)
    
    return a

def test_softmax():
    """
    Tests the custom softmax implementation with example inputs.
    """
    print("\n" + "=" * 70)
    print("Testing Custom Softmax Implementation")
    print("=" * 70)
    
    # Test 1: Simple example
    z1 = np.array([2.0, 1.0, 0.1])
    a1 = my_softmax(z1)
    print(f"\nTest 1:")
    print(f"  Input logits:  {z1}")
    print(f"  Probabilities: {a1}")
    print(f"  Sum: {np.sum(a1):.6f} (should be 1.0)")
    
    # Test 2: With negative values
    z2 = np.array([2.0, -1.0, 3.0])
    a2 = my_softmax(z2)
    print(f"\nTest 2:")
    print(f"  Input logits:  {z2}")
    print(f"  Probabilities: {a2}")
    print(f"  Predicted class: {np.argmax(a2)} (highest probability)")
    
    # Test 3: Batch processing
    z3 = np.array([[1.0, 2.0, 3.0, 4.0],
                   [1.0, 1.0, 1.0, 1.0]])
    a3 = my_softmax(z3)
    print(f"\nTest 3 (Batch):")
    print(f"  Input logits:\n{z3}")
    print(f"  Probabilities:\n{a3}")
    print(f"  Row sums: {np.sum(a3, axis=1)} (each should be 1.0)")

# =============================================================================
# SECTION 3: MODEL A - "OBVIOUS" APPROACH (Less Stable)
# =============================================================================

def build_model_a():
    """
    Model A: The "Obvious" but less stable approach.
    
    Architecture:
    -------------
    - Layer 1: Dense, 25 units, ReLU
    - Layer 2: Dense, 15 units, ReLU
    - Layer 3 (Output): Dense, 4 units, SOFTMAX
    
    Loss Function:
    --------------
    SparseCategoricalCrossentropy() - WITHOUT from_logits=True
    
    Why Less Stable?
    ----------------
    This approach computes softmax in the output layer, then computes
    cross-entropy loss. This involves TWO separate operations:
    
    1. Softmax: a = exp(z) / sum(exp(z))
    2. Cross-entropy: -log(a[true_class])
    
    Problem: When z values are large, exp(z) can overflow.
    When z values are very negative, exp(z) → 0, causing log(0) = -∞.
    
    Returns:
        model: Compiled Keras model
    """
    print("\n" + "=" * 70)
    print("MODEL A: 'Obvious' Approach (Softmax Activation)")
    print("=" * 70)
    
    model = Sequential([
        Dense(25, activation='relu', name='L1'),
        Dense(15, activation='relu', name='L2'),
        Dense(4, activation='softmax', name='L3_output')
    ], name='model_a_obvious')
    
    model.compile(
        loss=SparseCategoricalCrossentropy(),  # Expects probabilities
        optimizer=Adam(learning_rate=0.001),
        metrics=['accuracy']
    )
    
    print("\nModel A Architecture:")
    print("  • Output activation: SOFTMAX")
    print("  • Loss: SparseCategoricalCrossentropy()")
    print("  • Stability: LESS STABLE (two-step computation)")
    
    return model

# =============================================================================
# SECTION 4: MODEL B - "PREFERRED" APPROACH (More Stable)
# =============================================================================

def build_model_b():
    """
    Model B: The "Preferred" and more numerically stable approach.
    
    Architecture:
    -------------
    - Layer 1: Dense, 25 units, ReLU
    - Layer 2: Dense, 15 units, ReLU
    - Layer 3 (Output): Dense, 4 units, LINEAR
    
    Loss Function:
    --------------
    SparseCategoricalCrossentropy(from_logits=True)
    
    Why More Stable?
    ----------------
    This approach outputs LINEAR logits (raw scores), and the loss function
    internally combines softmax + cross-entropy into a single operation:
    
    Combined operation: -z[true_class] + log(sum(exp(z)))
    
    This uses the "log-sum-exp" trick to avoid computing large exponentials
    explicitly, preventing overflow/underflow issues.
    
    Mathematical Insight:
    ---------------------
    Instead of:
      a = softmax(z)
      loss = -log(a[true_class])
      
    We compute directly:
      loss = -z[true_class] + log(sum(exp(z)))
      
    Using the identity:
      log(exp(z_i) / sum(exp(z))) = z_i - log(sum(exp(z)))
    
    Returns:
        model: Compiled Keras model
    """
    print("\n" + "=" * 70)
    print("MODEL B: 'Preferred' Approach (Linear + from_logits=True)")
    print("=" * 70)
    
    model = Sequential([
        Dense(25, activation='relu', name='L1'),
        Dense(15, activation='relu', name='L2'),
        Dense(4, activation='linear', name='L3_output')  # LINEAR, not softmax
    ], name='model_b_preferred')
    
    model.compile(
        loss=SparseCategoricalCrossentropy(from_logits=True),  # Expects logits
        optimizer=Adam(learning_rate=0.001),
        metrics=['accuracy']
    )
    
    print("\nModel B Architecture:")
    print("  • Output activation: LINEAR (outputs logits)")
    print("  • Loss: SparseCategoricalCrossentropy(from_logits=True)")
    print("  • Stability: MORE STABLE (combined computation)")
    
    return model

# =============================================================================
# SECTION 5: TRAINING AND COMPARISON
# =============================================================================

def train_models(X, y):
    """
    Trains both models and compares their performance.
    """
    print("\n" + "=" * 70)
    print("Training Models")
    print("=" * 70)
    
    # Build models
    model_a = build_model_a()
    model_b = build_model_b()
    
    # Train Model A
    print("\nTraining Model A (Obvious)...")
    history_a = model_a.fit(X, y, epochs=10, verbose=1, validation_split=0.2)
    
    # Train Model B
    print("\nTraining Model B (Preferred)...")
    history_b = model_b.fit(X, y, epochs=10, verbose=1, validation_split=0.2)
    
    # Compare final accuracies
    print("\n" + "=" * 70)
    print("Training Results Comparison")
    print("=" * 70)
    print(f"\nModel A (Obvious):")
    print(f"  Final Loss: {history_a.history['loss'][-1]:.4f}")
    print(f"  Final Accuracy: {history_a.history['accuracy'][-1]*100:.2f}%")
    print(f"  Val Accuracy: {history_a.history['val_accuracy'][-1]*100:.2f}%")
    
    print(f"\nModel B (Preferred):")
    print(f"  Final Loss: {history_b.history['loss'][-1]:.4f}")
    print(f"  Final Accuracy: {history_b.history['accuracy'][-1]*100:.2f}%")
    print(f"  Val Accuracy: {history_b.history['val_accuracy'][-1]*100:.2f}%")
    
    return model_a, model_b, history_a, history_b

# =============================================================================
# SECTION 6: PREDICTION AND LOGIT CONVERSION
# =============================================================================

def demonstrate_predictions(model_a, model_b, X):
    """
    Demonstrates how to convert logits to probabilities and make predictions.
    """
    print("\n" + "=" * 70)
    print("Prediction Comparison: Logits vs Probabilities")
    print("=" * 70)
    
    # Get 5 random samples
    indices = np.random.choice(len(X), 5, replace=False)
    X_sample = X[indices]
    
    # Model A predictions (already probabilities)
    predictions_a = model_a.predict(X_sample, verbose=0)
    
    # Model B predictions (logits - raw scores)
    logits_b = model_b.predict(X_sample, verbose=0)
    
    # Convert logits to probabilities using TensorFlow
    probabilities_b_tf = tf.nn.softmax(logits_b).numpy()
    
    # Convert logits to probabilities using our custom function
    probabilities_b_custom = my_softmax(logits_b)
    
    print("\nSample Predictions (first 3 examples):")
    print("=" * 70)
    
    for i in range(3):
        print(f"\nExample {i+1}:")
        print(f"  Model A (Softmax output): {predictions_a[i]}")
        print(f"  Model B (Logits):         {logits_b[i]}")
        print(f"  Model B → Probabilities:  {probabilities_b_tf[i]}")
        print(f"  Custom Softmax:           {probabilities_b_custom[i]}")
        
        # Get predicted classes
        class_a = np.argmax(predictions_a[i])
        class_b = np.argmax(logits_b[i])
        
        print(f"  Predicted class (A): {class_a}")
        print(f"  Predicted class (B): {class_b}")
    
    # Demonstrate that argmax works directly on logits
    print("\n" + "-" * 70)
    print("Key Insight: argmax() doesn't need probabilities!")
    print("-" * 70)
    print("\nFor classification, you can use argmax() directly on logits:")
    print("  • argmax(logits) = argmax(softmax(logits))")
    print("  • Softmax is monotonic, so the highest logit → highest probability")
    print("  • This saves computation if you only need the class prediction")
    print("\nExample:")
    print(f"  Logits:       {logits_b[0]}")
    print(f"  argmax(logits) = {np.argmax(logits_b[0])}")
    print(f"  Probabilities: {probabilities_b_tf[0]}")
    print(f"  argmax(probs)  = {np.argmax(probabilities_b_tf[0])}")
    print(f"  → Same class! No need to compute softmax for predictions.")

# =============================================================================
# SECTION 7: VISUALIZATION
# =============================================================================

def plot_training_comparison(history_a, history_b):
    """
    Plots training history comparison between both models.
    """
    print("\n" + "=" * 70)
    print("Generating Training Comparison Plots")
    print("=" * 70)
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Loss comparison
    axes[0].plot(history_a.history['loss'], label='Model A (Obvious)', linewidth=2)
    axes[0].plot(history_b.history['loss'], label='Model B (Preferred)', linewidth=2, linestyle='--')
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('Loss', fontsize=12)
    axes[0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Accuracy comparison
    axes[1].plot(history_a.history['accuracy'], label='Model A (Obvious)', linewidth=2)
    axes[1].plot(history_b.history['accuracy'], label='Model B (Preferred)', linewidth=2, linestyle='--')
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('Accuracy', fontsize=12)
    axes[1].set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('softmax_training_comparison.png', dpi=150, bbox_inches='tight')
    print("  ✓ Saved comparison plot to 'softmax_training_comparison.png'")
    
    plt.show()

# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    """
    Main execution pipeline.
    """
    # Step 1: Create and save dataset
    X, y = create_and_save_dataset()
    
    # Step 2: Load dataset
    X, y = load_dataset()
    
    # Step 3: Test custom softmax
    test_softmax()
    
    # Step 4: Train both models
    model_a, model_b, history_a, history_b = train_models(X, y)
    
    # Step 5: Demonstrate predictions
    demonstrate_predictions(model_a, model_b, X)
    
    # Step 6: Visualize comparison
    plot_training_comparison(history_a, history_b)
    
    print("\n" + "=" * 70)
    print("✓ Softmax Multiclass Classification Complete!")
    print("=" * 70)
    
    print("\n" + "-" * 70)
    print("Key Takeaways:")
    print("-" * 70)
    print("1. Model B (from_logits=True) is MORE NUMERICALLY STABLE")
    print("2. Use linear activation in output layer + from_logits=True")
    print("3. Softmax couples all outputs together (unlike ReLU/Sigmoid)")
    print("4. For predictions, argmax() works directly on logits - no softmax needed")
    print("5. SparseCategoricalCrossentropy: integer labels (memory efficient)")
    print("6. CategoricalCrossentropy: one-hot encoded labels (less efficient)")
    print("-" * 70)
    
    print("\n" + "-" * 70)
    print("When to Use What:")
    print("-" * 70)
    print("• Training: Use linear output + SparseCategoricalCrossentropy(from_logits=True)")
    print("• Probabilities: Apply tf.nn.softmax() to logits when needed")
    print("• Classification: Use np.argmax() directly on logits")
    print("=" * 70)

if __name__ == "__main__":
    main()
